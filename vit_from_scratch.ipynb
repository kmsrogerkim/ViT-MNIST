{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9e55aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb847fc2",
   "metadata": {},
   "source": [
    "# 1. ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f96da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "PATCH_SIZE = 4\n",
    "EMB_DIM = 192\n",
    "\n",
    "IMG_SIZE = 7\n",
    "IMG_DEPTH = 1\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "NUM_HEADS = 4\n",
    "HIDDEN_DIM = 100\n",
    "DROP_OUT_RATE = 0.3\n",
    "\n",
    "WD = 0\n",
    "LR = 1e-4\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CSV_PATH = \"./data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2116d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, img_size, in_channel, emb_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.in_channel = in_channel\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.emb_layer = self._embadding_layer()\n",
    "\n",
    "        num_patch = (img_size // patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, num_patch, emb_dim))\n",
    "\n",
    "    def _embadding_layer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(kernel_size=self.patch_size, stride=self.patch_size,\n",
    "                      in_channels=self.in_channel, out_channels=self.emb_dim),\n",
    "            # (B, EMB_DIM, img_size / PATCH_SIZE , img_size / PATCH_SIZE)\n",
    "            nn.Flatten(start_dim=2) \n",
    "            # (B, EMB_DIM, 49)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb_layer(x).transpose(1,2)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(BATCH_SIZE, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # (B, 50, EMB_DIM)\n",
    "        x = x + self.pos_emb\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee15bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, hidden_dim, drop_out_rate, \n",
    "                 img_size, patch_size, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim        \n",
    "        self.num_heads = num_heads\n",
    "        self.droptout = drop_out_rate\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = PatchEmbedding(patch_size=patch_size, img_size=img_size, \n",
    "                                        in_channel=in_channels, emb_dim=emb_dim)\n",
    "        self.encoder = self._make_encoder_layers()\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.mlp_head = nn.Linear(emb_dim, num_classes)\n",
    "    \n",
    "    def _make_encoder_layers(self):\n",
    "        return nn.Sequential(\n",
    "            nn.TransformerEncoderLayer(d_model=self.emb_dim, nhead=self.num_heads,\n",
    "                                       dim_feedforward=self.hidden_dim, \n",
    "                                       dropout=self.droptout,\n",
    "                                       batch_first=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        # (B, 50, EMB_DIM)\n",
    "        cls_token = x[:, 0, :] # (B, EMB_DIM)\n",
    "        return self.mlp_head(cls_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c97896",
   "metadata": {},
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "743b2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        label = int(row['label'])\n",
    "        pixels = torch.tensor(row.iloc[1:].values, dtype=torch.float32)\n",
    "        image = pixels.view(1, 28, 28) \n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af3f5194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = Dataset(CSV_PATH)\n",
    "print(len(dataset))\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "278da00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_vit = ViT(EMB_DIM, NUM_HEADS, HIDDEN_DIM, DROP_OUT_RATE, IMG_SIZE, PATCH_SIZE, \n",
    "          in_channels=IMG_DEPTH, num_classes=NUM_CLASSES)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for image, label in loader:\n",
    "    out = test_vit(image)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364b680",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f07bf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "641df452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e40a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn, show_progress=False):\n",
    "    loop = tqdm(train_loader, leave=True) if show_progress else train_loader\n",
    "    losses = []\n",
    "\n",
    "    for _, (img, label) in enumerate(loop):\n",
    "        img = img.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        out = model(img)\n",
    "\n",
    "        loss = loss_fn(out, label)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if show_progress:\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ab2c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loader, model, loss_fn, show_progress=False):\n",
    "    model.eval()\n",
    "    loop = tqdm(loader, leave=True) if show_progress else loader\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (img, label) in enumerate(loop):\n",
    "            img = img.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            out = model(img)\n",
    "            loss = loss_fn(out, label)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if show_progress:\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "617d7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(EMB_DIM, NUM_HEADS, HIDDEN_DIM, DROP_OUT_RATE, IMG_SIZE, PATCH_SIZE, \n",
    "          in_channels=IMG_DEPTH, num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12783731",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset, val_small_dataset, _ = random_split(dataset, [3000, 600, len(dataset) - 3600])\n",
    "small_loader = torch.utils.data.DataLoader(small_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_small_loader = torch.utils.data.DataLoader(val_small_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b4763c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|‚ñè         | 10/500 [00:11<09:06,  1.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m    x = x.to(DEVICE)\n\u001b[32m     13\u001b[39m train_loss = train_fn(small_loader, model, optim, loss)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m val_loss = \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_small_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m train_losses.append(train_loss)\n\u001b[32m     16\u001b[39m val_losses.append(val_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mvalidate\u001b[39m\u001b[34m(loader, model, loss_fn, show_progress)\u001b[39m\n\u001b[32m      4\u001b[39m losses = []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/other/digit-recognizer/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/other/digit-recognizer/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/other/digit-recognizer/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/other/digit-recognizer/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m      9\u001b[39m row = \u001b[38;5;28mself\u001b[39m.data.iloc[idx]\n\u001b[32m     11\u001b[39m label = \u001b[38;5;28mint\u001b[39m(row[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m pixels = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m image = pixels.view(\u001b[32m1\u001b[39m, \u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m) \n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = None\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Training\"):\n",
    "    for x, y in small_loader:\n",
    "       x = x.to(DEVICE)\n",
    "\n",
    "    train_loss = train_fn(small_loader, model, optim, loss)\n",
    "    val_loss = validate(val_small_loader, model, loss)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"best_model_epoch{epoch + 1}.pt\")\n",
    "\n",
    "        if best_model_path is not None and os.path.exists(best_model_path):\n",
    "            os.remove(best_model_path)\n",
    "\n",
    "        best_model_path = f\"best_model_epoch{epoch + 1}.pt\"\n",
    "        # print(f\"New best model saved at epoch {epoch + 1} with val_loss = {val_loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('curve.png') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digit-recognizer-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
